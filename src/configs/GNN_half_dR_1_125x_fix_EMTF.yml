data: #Dataset-specific options. If these change, you must delete the previous processed files.
  data_dir: ../data
  log_dir: ../data/logs # Be careful to change this
  signal_dataset: 'DsTau3Mu.pkl'
  bkg_dataset: 'minbias.pkl'
  add_self_loops: True
  only_one_tau: True
  splits: # Fractions of data to use for training, validation, and testing
    train: 0.7
    valid: 0.15
    test: 0.15
  pos_neg_ratio: 0.0 # Ratio of signal samples to background samples. Use 0 to use as much signal/background as possible.
  radius: False
  eta_thresh: 0.5
  node_clf: False # Keep false.
  virtual_node: True # Virtual node in graph construction
  phi_transform: False # Keep false. This transforms node-feature 'mu_hit_sim_phi' to 'mu_hit_sim_cos_phi' and 'mu_hit_sim_sin_phi'
  far_station: False # This enables inter-station edges spanning more than one station.

  conditions:

  node_feature_names: # Variables to use for nodes
    - stub_real_eta1
    - stub_real_eta2
    - stub_real_phi1
    - stub_real_phi2
    - stub_tfLayer
  edge_feature_names: # Variables to use for edges. Feature calculated as node_feature[i] - node_feature[j]
    - stub_real_eta1
    - stub_real_eta2
    - stub_real_phi1
    - stub_real_phi2
    - stub_tfLayer


model: # Model-specific options.
  bn_input: True # Apply batch-norm on the input
  n_layers: 6 # Number of hidden layers
  out_channels: 256 # Dimension of hidden layers
  dropout_p: 0.1 # Dropout Probability
  readout: pool # Global readout. Keep pool for now
  norm_type: batch # Norm between layers. Keep batch for now
  deepgcn_aggr: softmax # Aggregation function used in GENConv layer.
  conv_type: GENConv # Convolution layer to use.
  skip_num: 1 # Number of residual connections
  edge_atten: False # Introduce additional parameters to weight edge-features in each layer
  endcap: -1 # Choice of endcap. 1 = Positive Endcap, -1 = Negative Endcap
  weight_bit_width: 6
  
eval:
  test_interval: 5 # How often to evaluate on test-dataset
  auroc_max_fpr: 0.001

optimizer: # Optimization options
  resume: False
  lr: 1.0e-3 # Learning rate of AdamW algorithm
  batch_size: 512 # Number of samples to use in each batch
  epochs: 200 # Number of epochs to train for
  focal_loss: True # Loss function 
  focal_alpha: 0.4 # Loss function parameter
  focal_gamma: 0. # Loss function parameter
